{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
        "<br> <br>\n",
    "- It is recommended to use NLTK for this classification task, as the features stored in dictionary style can be easily extended. While scikit-learn is easier for Q2, it might not be that straightforward to combine different features in Q3. In addition, dealing with categorical variables can be a pain in scikit-learn. If you plan to use scikit-learn anyway, please read the following post: http://pbpython.com/categorical-encoding.html\n",
    "- While protyping, it is easier to stick to the Naive Bayes Classifier. Adding other classifiers once your code is bug-free.\n",
    "- Use cross validation on the training set to avoid over-fitting, though it is not guaranteed achieve that purpose.\n",
    "\n",
    "\n",
    "<br>\n",
    "This notebooks involves the following tasks:\n",
    "- Construct features from strings (i.e., usernames)\n",
    "- Frequent use of zip() and zip(*) (see doc https://docs.python.org/3/library/functions.html)\n",
    "- Parsing a json style column into multiple columns\n",
    "- Merging different features into one feature set\n",
    "- Find appropriate models and features to improve prediction accuracy\n",
    "- Writing and debugging a lot of code\n",
    "<br><br>\n",
    "\n",
     "<br><br>\n",
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import re\n",
    "import nltk\n",
    "os.chdir(r'C:\\Users\\Varni\\Desktop\\Course info\\Python\\assignment_archive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train = pd.read_csv('train.csv')\n",
    "test = pd.read_csv('test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Just for the purpose of feat experimentation\n",
    "train_init = train[:4500]\n",
    "dev_test= train[4500:6249]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Predicting Gender with Username\n",
    "Some potential features of usernames: whether it has capital letters, whether it has digits, number of characters, number of vowels, first and last letters, etc. See http://www.nltk.org/book/ch06.html for some related code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def vowels(x):\n",
    "        count =0 \n",
    "        for i in x.lower():\n",
    "                if i in ['a','e','i','o','u']:\n",
    "                    count += 1\n",
    "        return count\n",
    "\n",
    "def digits_in(x):\n",
    "    count =0 \n",
    "    for i in x.lower():\n",
    "                if i in ['0','1','2','3','4','5','6','7','8','9']:\n",
    "                    count += 1\n",
    "    return count\n",
    "\n",
    "def name_features(word):\n",
    "    features={}\n",
    "    features[\"first_letter\"]=word[0]\n",
    "    features[\"last_letter\"]=word[-1]\n",
    "    features[\"upper_cnt\"]=sum(map(str.isupper, word))\n",
    "    features[\"lower_cnt\"]=sum(map(str.islower, word))\n",
    "    features[\"hasupper\"]=word.isupper()\n",
    "    features[\"length\"]=len(word)\n",
    "    features[\"vowel_count\"]= vowels(word)\n",
    "    features['digits_in']=digits_in(word)\n",
    "    #features['prefix']= word[0:2]\n",
    "    #features['suffix2']= word[-2:]\n",
    "    vowel = ('a','e','i','o','u','A','E','I','O','U')\n",
    "    if(re.sub('[^A-Za-z0-9]+', '', word)):\n",
    "        features['endswithvowel'] = (re.sub(r'[0-9]+', '', word)).endswith(vowel)\n",
    "        return features\n",
    "\n",
    "def name_features2(word):\n",
    "    features={}\n",
    "    features[\"first_letter\"]=word[0]\n",
    "    features[\"last_letter\"]=word[-1]\n",
    "    features[\"upper_cnt\"]=sum(map(str.isupper, word))\n",
    "    features[\"lower_cnt\"]=sum(map(str.islower, word))\n",
    "    features[\"length\"]=len(word)\n",
    "    features[\"vowel_count\"]= vowels(word)\n",
    "    features['digits_in']=digits_in(word)\n",
    "    features['prefix']= word[0:2]\n",
    "    features['suffix2']= word[-2:]\n",
    "    vowel = ('a','e','i','o','u','A','E','I','O','U')\n",
    "    if(re.sub('[^A-Za-z0-9]+', '', word)):\n",
    "        features['endswithvowel'] = (re.sub(r'[0-9]+', '', word)).endswith(vowel)\n",
    "        return features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'first_letter': 'p', 'last_letter': 'E', 'upper_cnt': 4, 'lower_cnt': 5, 'hasupper': False, 'length': 10, 'vowel_count': 4, 'digits_in': 1, 'endswithvowel': True}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'digits_in': 0,\n",
       " 'endswithvowel': False,\n",
       " 'first_letter': 'p',\n",
       " 'last_letter': 'Y',\n",
       " 'length': 8,\n",
       " 'lower_cnt': 5,\n",
       " 'prefix': 'pa',\n",
       " 'suffix2': 'HY',\n",
       " 'upper_cnt': 3,\n",
       " 'vowel_count': 3}"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(name_features('paul3iWHYE'))\n",
    "name_features2('pauliWHY')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7667238421955404\n",
      "Most Informative Features\n",
      "                  prefix = 'vi'                M : F      =     10.0 : 1.0\n",
      "                  prefix = 'Wo'                F : M      =      9.9 : 1.0\n",
      "                  prefix = 'Ai'                F : M      =      7.7 : 1.0\n",
      "                  prefix = 'El'                F : M      =      7.7 : 1.0\n",
      "                  prefix = 'Te'                F : M      =      7.7 : 1.0\n"
     ]
    }
   ],
   "source": [
    "#Trials for new features\n",
    "train_set = [(name_features2(a), b) for (a,b) in train_init.iloc[:,0:2].itertuples(index=False)]\n",
    "dev_test_set = [(name_features2(a), b) for (a,b) in dev_test.iloc[:,0:2].itertuples(index=False)]\n",
    "\n",
    "# Naive Bayes to detect any new features\n",
    "classifier = nltk.NaiveBayesClassifier.train(train_set)\n",
    "print(nltk.classify.accuracy(classifier, dev_test_set))\n",
    "classifier.show_most_informative_features(5)\n",
    "# Print errors\n",
    "errors = []\n",
    "for (a,b) in dev_test.iloc[:,0:2].itertuples(index=False):\n",
    "    guess = classifier.classify(name_features2(a))\n",
    "    if guess != b:\n",
    "          errors.append((b, guess, a))\n",
    "#errors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'M'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classifier.classify(name_features2('Vistry'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 561,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('M', 'F', 'SniperA51'),\n",
       " ('F', 'M', 'anaroxyl'),\n",
       " ('F', 'M', 'anita82'),\n",
       " ('F', 'M', 'ap2studio'),\n",
       " ('F', 'M', 'dreamtezz'),\n",
       " ('F', 'M', 'yuki007'),\n",
       " ('F', 'M', 'silvercat80'),\n",
       " ('F', 'M', 'rosalinbd'),\n",
       " ('F', 'M', 'quyan213'),\n",
       " ('M', 'F', 'TechVillage'),\n",
       " ('M', 'F', 'cucinastanza'),\n",
       " ('F', 'M', 'traducci'),\n",
       " ('F', 'M', 'damphat'),\n",
       " ('F', 'M', 'mradcliff'),\n",
       " ('F', 'M', 'csiii'),\n",
       " ('F', 'M', 'lskuro14'),\n",
       " ('F', 'M', 'HelenWu618'),\n",
       " ('F', 'M', 'GosthMan'),\n",
       " ('F', 'M', 'jniles'),\n",
       " ('F', 'M', 'MagentoKing'),\n",
       " ('F', 'M', 'yoka'),\n",
       " ('F', 'M', 'rairamirez'),\n",
       " ('F', 'M', 'melodyann1983'),\n",
       " ('F', 'M', 'melcurick'),\n",
       " ('M', 'F', 'shreeinfosolutio'),\n",
       " ('F', 'M', 'webprachi'),\n",
       " ('F', 'M', 'JananiKrishnan'),\n",
       " ('F', 'M', 'Samantha6'),\n",
       " ('F', 'M', 'csapaugh'),\n",
       " ('M', 'F', 'MirrorDevelopers'),\n",
       " ('F', 'M', 'sand235'),\n",
       " ('F', 'M', 'tecknare'),\n",
       " ('F', 'M', 'karlatapia'),\n",
       " ('M', 'F', 'MuhammadFahim85'),\n",
       " ('M', 'F', 'Ecoserve'),\n",
       " ('F', 'M', 'jennparmex'),\n",
       " ('F', 'M', 'bift'),\n",
       " ('F', 'M', 'MBodman88'),\n",
       " ('F', 'M', 'maricrisabellon'),\n",
       " ('F', 'M', 'annespeak'),\n",
       " ('F', 'M', 'kennylukhk'),\n",
       " ('F', 'M', 'kristinwhite'),\n",
       " ('F', 'M', 'bmushungwasha'),\n",
       " ('F', 'M', 'Nancilynn'),\n",
       " ('F', 'M', 'ladybugwriter'),\n",
       " ('F', 'M', 'martabieluch'),\n",
       " ('F', 'M', 'ademel08'),\n",
       " ('F', 'M', 'fabiaf'),\n",
       " ('F', 'M', 'merguerette'),\n",
       " ('F', 'M', 'iiv'),\n",
       " ('F', 'M', 'jsmulson'),\n",
       " ('F', 'M', 'arryacreatives'),\n",
       " ('F', 'M', 'manojenni'),\n",
       " ('F', 'M', 'tharris98'),\n",
       " ('F', 'M', 'nadrainam'),\n",
       " ('F', 'M', 'amaliacorlan'),\n",
       " ('F', 'M', 'shortygurl03'),\n",
       " ('F', 'M', 'zackire'),\n",
       " ('M', 'F', 'FilipRadenovski'),\n",
       " ('M', 'F', 'TwinTurboStudios'),\n",
       " ('F', 'M', 'TrisKreuzer'),\n",
       " ('F', 'M', 'asialyn'),\n",
       " ('F', 'M', 'jgdp'),\n",
       " ('F', 'M', 'ButlerTL'),\n",
       " ('F', 'M', 'bebecita'),\n",
       " ('F', 'M', 'disuggests'),\n",
       " ('F', 'M', 'Veronique30'),\n",
       " ('F', 'M', 'jcpt1984'),\n",
       " ('F', 'M', 'teraleaman'),\n",
       " ('F', 'M', 'erikalich'),\n",
       " ('F', 'M', 'laurapagan'),\n",
       " ('F', 'M', 'yoonmi'),\n",
       " ('F', 'M', 'sheenabehot'),\n",
       " ('F', 'M', 'jraultsmith'),\n",
       " ('F', 'M', 'mekana79'),\n",
       " ('F', 'M', 'visualmoments'),\n",
       " ('F', 'M', 'haidiah'),\n",
       " ('F', 'M', 'Everud'),\n",
       " ('F', 'M', 'businesswoman'),\n",
       " ('F', 'M', 'rashmikandiyal'),\n",
       " ('F', 'M', 'pennyHu'),\n",
       " ('F', 'M', 'technoqueries'),\n",
       " ('F', 'M', 'jdriller'),\n",
       " ('M', 'F', 'EamonnDH'),\n",
       " ('F', 'M', 'seofortunate'),\n",
       " ('F', 'M', 'colancevska'),\n",
       " ('F', 'M', 'trapham'),\n",
       " ('F', 'M', 'taylorn3d'),\n",
       " ('F', 'M', 'ncmonroe1981'),\n",
       " ('F', 'M', 'arpitagain'),\n",
       " ('F', 'M', 'captiveye'),\n",
       " ('F', 'M', 'cbarnette'),\n",
       " ('F', 'M', 'LeoMom3'),\n",
       " ('F', 'M', 'chikanele'),\n",
       " ('F', 'M', 'preetishrivastav'),\n",
       " ('F', 'M', 'latika07'),\n",
       " ('M', 'F', 'JFitzDela'),\n",
       " ('F', 'M', 'govern'),\n",
       " ('F', 'M', 'sicibanez'),\n",
       " ('F', 'M', 'togle1'),\n",
       " ('F', 'M', 'PriyaJain6447'),\n",
       " ('F', 'M', 'quickinscription'),\n",
       " ('F', 'M', 'johana2242'),\n",
       " ('F', 'M', 'shikha01'),\n",
       " ('F', 'M', 'DJBVentures'),\n",
       " ('F', 'M', 'calimanr'),\n",
       " ('F', 'M', 'saszka3d'),\n",
       " ('F', 'M', 'tahmina8765'),\n",
       " ('F', 'M', 'joonichia84'),\n",
       " ('M', 'F', 'Tomzi'),\n",
       " ('F', 'M', 'Sofronieva'),\n",
       " ('F', 'M', 'hegelruiz'),\n",
       " ('F', 'M', 'chinafreelance'),\n",
       " ('F', 'M', 'stellarista'),\n",
       " ('F', 'M', 'Merr'),\n",
       " ('F', 'M', 'itexpart'),\n",
       " ('F', 'M', 'hilaryisabel'),\n",
       " ('F', 'M', 'agp3creativeseed'),\n",
       " ('F', 'M', 'jimenama'),\n",
       " ('F', 'M', 'loti21'),\n",
       " ('F', 'M', 'peyttimang'),\n",
       " ('M', 'F', 'Coffeemachine'),\n",
       " ('F', 'M', 'shalikau'),\n",
       " ('M', 'F', 'KISLAYB'),\n",
       " ('F', 'M', 'Adriana22'),\n",
       " ('M', 'F', 'prakasha14683'),\n",
       " ('F', 'M', 'novaguerrero'),\n",
       " ('F', 'M', 'MaryMadoz'),\n",
       " ('F', 'M', 'lgibson'),\n",
       " ('M', 'F', 'InspireVideo'),\n",
       " ('F', 'M', 'monikatrikha'),\n",
       " ('M', 'F', 'Shova055'),\n",
       " ('M', 'F', 'Jobinho5'),\n",
       " ('M', 'F', 'PaTTomousPRIME'),\n",
       " ('F', 'M', 'NamuunBat'),\n",
       " ('F', 'M', 'Tomajw'),\n",
       " ('F', 'M', 'anita07'),\n",
       " ('F', 'M', 'BADBIKER'),\n",
       " ('M', 'F', 'mea'),\n",
       " ('F', 'M', 'paulafrog'),\n",
       " ('F', 'M', 'Shaqwuanna'),\n",
       " ('F', 'M', 'lboogie38'),\n",
       " ('M', 'F', 'WebProf88'),\n",
       " ('F', 'M', 'twocats'),\n",
       " ('F', 'M', 'spypuzpa'),\n",
       " ('M', 'F', 'muhammadziaurzia'),\n",
       " ('M', 'F', 'TCRabelo'),\n",
       " ('F', 'M', 'bumblebee666'),\n",
       " ('F', 'M', 'writerprincess91'),\n",
       " ('F', 'M', 'chapot'),\n",
       " ('F', 'M', 'daisies1062'),\n",
       " ('F', 'M', 'asprof'),\n",
       " ('F', 'M', 'apurvachitte'),\n",
       " ('F', 'M', 'chikitita'),\n",
       " ('F', 'M', 'angelyn101'),\n",
       " ('F', 'M', 'raveenaskill'),\n",
       " ('F', 'M', 'qshahnawaz'),\n",
       " ('F', 'M', 'hollifischer'),\n",
       " ('F', 'M', 'iDivya'),\n",
       " ('F', 'M', 'michellewhited'),\n",
       " ('F', 'M', 'orangereya'),\n",
       " ('F', 'M', 'jctmit17'),\n",
       " ('M', 'F', 'KELLENKASH'),\n",
       " ('F', 'M', 'tuyakiki'),\n",
       " ('F', 'M', 'powlink'),\n",
       " ('F', 'M', 'etarkus'),\n",
       " ('F', 'M', 'diponchi'),\n",
       " ('F', 'M', 'jenndesign'),\n",
       " ('F', 'M', 'heidisutherlin'),\n",
       " ('F', 'M', 'eviaa'),\n",
       " ('F', 'M', 'triciaran'),\n",
       " ('M', 'F', 'StasDB'),\n",
       " ('F', 'M', 'Shamima123'),\n",
       " ('F', 'M', 'Downwindz'),\n",
       " ('F', 'M', 'dvori'),\n",
       " ('F', 'M', 'colorin46'),\n",
       " ('F', 'M', 'anniecleofas'),\n",
       " ('F', 'M', 'cindyabad'),\n",
       " ('F', 'M', 'peregra'),\n",
       " ('F', 'M', 'hellowas07'),\n",
       " ('M', 'F', 'Ratan50000000000'),\n",
       " ('F', 'M', 'maconsueloflores'),\n",
       " ('F', 'M', 'silvercat80'),\n",
       " ('F', 'M', 'Stefany93'),\n",
       " ('F', 'M', 'hailedcarrot'),\n",
       " ('F', 'M', 'cryslyn08'),\n",
       " ('F', 'M', 'beccahubbard'),\n",
       " ('F', 'M', 'lisenkotanya'),\n",
       " ('F', 'M', 'freelancerva'),\n",
       " ('M', 'F', 'Irfan20012'),\n",
       " ('F', 'M', 'jsierakowska'),\n",
       " ('F', 'M', 'pureninavclar644'),\n",
       " ('F', 'M', 'Barbiq'),\n",
       " ('F', 'M', 'jgoldman'),\n",
       " ('F', 'M', 'amber1210'),\n",
       " ('F', 'M', 'mariamg'),\n",
       " ('F', 'M', 'denmother'),\n",
       " ('F', 'M', 'nerryanne'),\n",
       " ('F', 'M', 'SweetyRawani'),\n",
       " ('M', 'F', 'GaurG'),\n",
       " ('F', 'M', 'bonny99'),\n",
       " ('F', 'M', 'Polly224'),\n",
       " ('F', 'M', 'LeoMom3'),\n",
       " ('F', 'M', 'SamMitchell'),\n",
       " ('F', 'M', 'janecaputolan'),\n",
       " ('F', 'M', 'timelyperfection'),\n",
       " ('M', 'F', 'malithafernando'),\n",
       " ('F', 'M', 'AlisaAspero'),\n",
       " ('F', 'M', 'ozgegokmen'),\n",
       " ('M', 'F', 'JohnMatta'),\n",
       " ('F', 'M', 'obriones'),\n",
       " ('F', 'M', 'eShalSoft'),\n",
       " ('F', 'M', 'lindawrites63'),\n",
       " ('F', 'M', 'jloar'),\n",
       " ('F', 'M', 'lesnbek'),\n",
       " ('F', 'M', 'kristalyn15'),\n",
       " ('F', 'M', 'emiglia'),\n",
       " ('F', 'M', 'Elithabeth'),\n",
       " ('F', 'M', 'angelinlin'),\n",
       " ('F', 'M', 'thesa'),\n",
       " ('F', 'M', 'SheilaVG'),\n",
       " ('F', 'M', 'issamia82'),\n",
       " ('F', 'M', 'amybaby'),\n",
       " ('F', 'M', 'vikasseo123'),\n",
       " ('F', 'M', 'yuktic'),\n",
       " ('M', 'F', 'BillyF22'),\n",
       " ('F', 'M', 'lilianunogwu'),\n",
       " ('M', 'F', 'Gm2009Software'),\n",
       " ('F', 'M', 'komi4545'),\n",
       " ('F', 'M', 'salye'),\n",
       " ('F', 'M', 'Shoegal02011984'),\n",
       " ('F', 'M', 'jmp88'),\n",
       " ('F', 'M', 'ileanabulina'),\n",
       " ('F', 'M', 'designerartist'),\n",
       " ('M', 'F', 'MVAGraphics1983'),\n",
       " ('M', 'F', 'GoutamKamala1976'),\n",
       " ('F', 'M', 'sandyjparrish'),\n",
       " ('F', 'M', 'sofiavarano'),\n",
       " ('F', 'M', 'ayaans'),\n",
       " ('F', 'M', 'DIMIND'),\n",
       " ('F', 'M', 'jnyce78'),\n",
       " ('F', 'M', 'dhati'),\n",
       " ('F', 'M', 'Ladyne'),\n",
       " ('F', 'M', 'skydel1498'),\n",
       " ('F', 'M', 'nadsma'),\n",
       " ('F', 'M', 'rochellef'),\n",
       " ('M', 'F', 'Naidu0921'),\n",
       " ('F', 'M', 'junnahjob'),\n",
       " ('M', 'F', 'saikatKolkata'),\n",
       " ('F', 'M', 'mslisalisa'),\n",
       " ('F', 'M', 'sybilleruth'),\n",
       " ('F', 'M', 'niki84'),\n",
       " ('F', 'M', 'Vistry'),\n",
       " ('F', 'M', 'epaytas'),\n",
       " ('F', 'M', 'eeyore1'),\n",
       " ('F', 'M', 'ceb291'),\n",
       " ('F', 'M', 'wvcopywriter'),\n",
       " ('F', 'M', 'MaxiVelasco'),\n",
       " ('M', 'F', 'Balajinaidu'),\n",
       " ('M', 'F', 'Ecoserve'),\n",
       " ('F', 'M', 'pomnett'),\n",
       " ('F', 'M', 'GypsyPunk'),\n",
       " ('F', 'M', 'wuraola2'),\n",
       " ('F', 'M', 'amzcastaneda'),\n",
       " ('F', 'M', 'mafka'),\n",
       " ('M', 'F', 'IanSmithISA'),\n",
       " ('F', 'M', 'clsoftware'),\n",
       " ('F', 'M', 'aicxmedina'),\n",
       " ('F', 'M', 'yuo'),\n",
       " ('F', 'M', 'djkickass'),\n",
       " ('F', 'M', 'wadehjb'),\n",
       " ('F', 'M', 'PRosenthal'),\n",
       " ('F', 'M', 'mkoutsikou'),\n",
       " ('F', 'M', 'mackiejp'),\n",
       " ('F', 'M', 'hayoomaa'),\n",
       " ('F', 'M', 'jahkey'),\n",
       " ('F', 'M', 'sanal0723'),\n",
       " ('F', 'M', 'tempsolution'),\n",
       " ('F', 'M', 'TILLYP'),\n",
       " ('F', 'M', 'maryfaithlopoz'),\n",
       " ('M', 'F', 'Gravedistraction'),\n",
       " ('F', 'M', 'MariaEvK'),\n",
       " ('F', 'M', 'kopila123'),\n",
       " ('F', 'M', 'kabil1'),\n",
       " ('F', 'M', 'auyaco'),\n",
       " ('M', 'F', 'muralimohanneela'),\n",
       " ('F', 'M', 'easac'),\n",
       " ('F', 'M', 'Ger'),\n",
       " ('F', 'M', 'DebraF2010'),\n",
       " ('M', 'F', 'Bradg19'),\n",
       " ('M', 'F', 'Litonice07'),\n",
       " ('F', 'M', 'joeju'),\n",
       " ('F', 'M', 'MARTHALI11'),\n",
       " ('M', 'F', 'Yaroslav3D'),\n",
       " ('F', 'M', 'rex2moon'),\n",
       " ('F', 'M', 'nharvey1978'),\n",
       " ('F', 'M', 'monicabomengen'),\n",
       " ('F', 'M', 'veethitelang'),\n",
       " ('F', 'M', 'parulbidani'),\n",
       " ('F', 'M', 'glai01'),\n",
       " ('F', 'M', 'contentwriter82'),\n",
       " ('M', 'F', 'LSE'),\n",
       " ('F', 'M', 'ChristinaGrace'),\n",
       " ('F', 'M', 'rosearamos'),\n",
       " ('F', 'M', 'yetsog'),\n",
       " ('F', 'M', 'ardelfonso'),\n",
       " ('M', 'F', 'Mamadotheking09'),\n",
       " ('F', 'M', 'loreligwaps'),\n",
       " ('F', 'M', 'scarfeys'),\n",
       " ('F', 'M', 'paulafelix73'),\n",
       " ('M', 'F', 'Iqbal74'),\n",
       " ('F', 'M', 'meeshellyee'),\n",
       " ('M', 'F', 'Adrian11111'),\n",
       " ('F', 'M', 'sanafreddy'),\n",
       " ('F', 'M', 'januads'),\n",
       " ('F', 'M', 'designedbyjackie'),\n",
       " ('F', 'M', 'Grooviechick'),\n",
       " ('M', 'F', 'Enzo24'),\n",
       " ('F', 'M', 'armaningo'),\n",
       " ('F', 'M', 'amellor'),\n",
       " ('F', 'M', 'shanntech'),\n",
       " ('F', 'M', 'Northy'),\n",
       " ('F', 'M', 'RebeccaSuchi'),\n",
       " ('F', 'M', 'baoshanzhao'),\n",
       " ('M', 'F', 'Waqar013'),\n",
       " ('F', 'M', 'claudiestein'),\n",
       " ('F', 'M', 'LaurenCurtis'),\n",
       " ('F', 'M', 'candida'),\n",
       " ('F', 'M', 'myillustration'),\n",
       " ('F', 'M', 'rajimallik'),\n",
       " ('M', 'F', 'mistakenGrace'),\n",
       " ('F', 'M', 'mcworx'),\n",
       " ('F', 'M', 'bhavanajain'),\n",
       " ('M', 'F', 'DeepakAcharya68'),\n",
       " ('F', 'M', 'eagleseye1284'),\n",
       " ('F', 'M', 'sashamara'),\n",
       " ('F', 'M', 'MiriamR'),\n",
       " ('F', 'M', 'jairenavarro'),\n",
       " ('M', 'F', 'Gable22aa'),\n",
       " ('M', 'F', 'ajithkranatunga'),\n",
       " ('F', 'M', 'SunnyGault'),\n",
       " ('F', 'M', 'evaughan'),\n",
       " ('F', 'M', 'algie123'),\n",
       " ('F', 'M', 'ron2dmax'),\n",
       " ('M', 'F', 'Sihanoukville'),\n",
       " ('F', 'M', 'worker4Nat'),\n",
       " ('F', 'M', 'lcreurer'),\n",
       " ('M', 'F', 'Solomonkariri'),\n",
       " ('F', 'M', 'FlorGambartes')]"
      ]
     },
     "execution_count": 561,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "errors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Final feature set using username\n",
    "feat_set = [(name_features(a), b) for (a,b) in train.iloc[:,0:2].itertuples(index=False)]\n",
    "#Alternative\n",
    "feat_set_alt =  [(name_features2(a), b) for (a,b) in train.iloc[:,0:2].itertuples(index=False)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "({'first_letter': 's', 'last_letter': 'm', 'upper_cnt': 0, 'lower_cnt': 5, 'hasupper': False, 'length': 5, 'vowel_count': 2, 'digits_in': 0, 'endswithvowel': False}, 'M')\n",
      "({'first_letter': 's', 'last_letter': 'm', 'upper_cnt': 0, 'lower_cnt': 5, 'length': 5, 'vowel_count': 2, 'digits_in': 0, 'prefix': 'sh', 'suffix2': 'om', 'endswithvowel': False}, 'M')\n"
     ]
    }
   ],
   "source": [
    "print(feat_set[1])\n",
    "print(feat_set_alt[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy: 0.8072\n",
      "accuracy: 0.8024\n",
      "accuracy: 0.7832\n",
      "accuracy: 0.792\n",
      "accuracy: 0.811048839071257\n",
      "CV mean accuracy: 0.799169767814\n",
      "Most Informative Features\n",
      "               upper_cnt = 6                   F : M      =      7.7 : 1.0\n",
      "             last_letter = 'O'                 F : M      =      7.0 : 1.0\n",
      "            first_letter = 'Y'                 F : M      =      5.4 : 1.0\n",
      "            first_letter = 'X'                 F : M      =      4.2 : 1.0\n",
      "             last_letter = 'D'                 F : M      =      4.2 : 1.0\n",
      "             last_letter = 'J'                 F : M      =      4.2 : 1.0\n",
      "             last_letter = 'b'                 M : F      =      3.6 : 1.0\n",
      "            first_letter = 'u'                 M : F      =      3.0 : 1.0\n",
      "            first_letter = 'W'                 F : M      =      2.9 : 1.0\n",
      "            first_letter = 'q'                 F : M      =      2.7 : 1.0\n",
      "             last_letter = 'C'                 F : M      =      2.7 : 1.0\n",
      "               digits_in = 9                   F : M      =      2.6 : 1.0\n",
      "                  length = 3                   F : M      =      2.6 : 1.0\n",
      "               upper_cnt = 12                  F : M      =      2.6 : 1.0\n",
      "            first_letter = 'E'                 F : M      =      2.5 : 1.0\n"
     ]
    }
   ],
   "source": [
    "#Naive Bayes - Cross Validation\n",
    "import nltk\n",
    "from sklearn.model_selection import KFold\n",
    "import numpy as np\n",
    "k_fold = KFold(n_splits=5, shuffle=True)\n",
    "accu = []\n",
    "for train_idx, test_idx in k_fold.split(feat_set):\n",
    "    training = [feat_set[i] for i in train_idx]\n",
    "    testing = [feat_set[i] for i in test_idx]\n",
    "    classifier_nb = nltk.NaiveBayesClassifier.train(training)   \n",
    "    accu.append( nltk.classify.util.accuracy(classifier_nb, testing) )\n",
    "    print('accuracy:', accu[len(accu)-1])    \n",
    "print('CV mean accuracy:', np.mean(accu)) \n",
    "classifier_nb.show_most_informative_features(15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy: 0.8096\n",
      "accuracy: 0.8112\n",
      "accuracy: 0.8184\n",
      "accuracy: 0.7928\n",
      "accuracy: 0.8126501200960768\n",
      "CV mean accuracy: 0.808930024019\n",
      "Most Informative Features\n",
      "                  prefix = 'El'                F : M      =     12.3 : 1.0\n",
      "                  prefix = 'Je'                F : M      =     10.0 : 1.0\n",
      "                  prefix = 'wr'                F : M      =     10.0 : 1.0\n",
      "                 suffix2 = 'e1'                F : M      =     10.0 : 1.0\n",
      "             last_letter = 'O'                 F : M      =      9.8 : 1.0\n",
      "                 suffix2 = 'va'                F : M      =      9.1 : 1.0\n",
      "                  prefix = 'Ki'                F : M      =      7.8 : 1.0\n",
      "                 suffix2 = '30'                F : M      =      7.8 : 1.0\n",
      "                  prefix = 'ha'                M : F      =      6.1 : 1.0\n",
      "                  prefix = 'vi'                M : F      =      6.0 : 1.0\n",
      "                  prefix = 'my'                F : M      =      6.0 : 1.0\n",
      "                 suffix2 = 'jo'                F : M      =      6.0 : 1.0\n",
      "                  prefix = 'jg'                F : M      =      5.6 : 1.0\n",
      "                  prefix = 'dy'                F : M      =      5.6 : 1.0\n",
      "                  prefix = 'Sw'                F : M      =      5.6 : 1.0\n"
     ]
    }
   ],
   "source": [
    "#Naive Bayes - Cross Validation - Alternate feature set\n",
    "import nltk\n",
    "from sklearn.model_selection import KFold\n",
    "import numpy as np\n",
    "k_fold = KFold(n_splits=5, shuffle=True)\n",
    "accu = []\n",
    "for train_idx, test_idx in k_fold.split(feat_set_alt):\n",
    "    training = [feat_set_alt[i] for i in train_idx]\n",
    "    testing = [feat_set_alt[i] for i in test_idx]\n",
    "    classifier_nb_alt = nltk.NaiveBayesClassifier.train(training)   \n",
    "    accu.append( nltk.classify.util.accuracy(classifier_nb, testing) )\n",
    "    print('accuracy:', accu[len(accu)-1])    \n",
    "print('CV mean accuracy:', np.mean(accu)) \n",
    "classifier_nb_alt.show_most_informative_features(15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy: 0.8174539631705364\n",
      "accuracy: 0.8174539631705364\n",
      "accuracy: 0.8174539631705364\n",
      "accuracy: 0.8174539631705364\n",
      "accuracy: 0.8174539631705364\n",
      "CV mean accuracy: 0.817453963171\n"
     ]
    }
   ],
   "source": [
    "#Sk-learn\n",
    "from nltk.classify import SklearnClassifier\n",
    "from sklearn.svm import SVC\n",
    "k_fold = KFold(n_splits=5, shuffle=True)\n",
    "accu = []\n",
    "for train_idx, test_idx in k_fold.split(feat_set):\n",
    "    training = [feat_set[i] for i in train_idx]\n",
    "    testing = [feat_set[i] for i in test_idx]\n",
    "    classifier_sk = SklearnClassifier(SVC(kernel='linear', C=10, random_state=1), sparse=True).train(training)       \n",
    "    accu.append( nltk.classify.util.accuracy(classifier_sk, testing) )\n",
    "    print('accuracy:', accu[len(accu)-1])    \n",
    "print('CV mean accuracy:', np.mean(accu))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy: 0.7808\n",
      "accuracy: 0.7896\n",
      "accuracy: 0.772\n",
      "accuracy: 0.7776\n",
      "accuracy: 0.7710168134507606\n",
      "CV mean accuracy: 0.77820336269\n"
     ]
    }
   ],
   "source": [
    "#Alternative feature set using sk-learn\n",
    "from nltk.classify import SklearnClassifier\n",
    "from sklearn.svm import SVC\n",
    "k_fold = KFold(n_splits=5, shuffle=True)\n",
    "accu = []\n",
    "for train_idx, test_idx in k_fold.split(feat_set_alt):\n",
    "    training = [feat_set_alt[i] for i in train_idx]\n",
    "    testing = [feat_set_alt[i] for i in test_idx]\n",
    "    classifier_sk_alt1 = SklearnClassifier(SVC(kernel='linear', C=10, random_state=1), sparse=True).train(training)       \n",
    "    accu.append( nltk.classify.util.accuracy(classifier_sk_alt1, testing) )\n",
    "    print('accuracy:', accu[len(accu)-1])    \n",
    "print('CV mean accuracy:', np.mean(accu))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8068\n",
      "0.813\n",
      "0.8626\n",
      "0.817\n"
     ]
    }
   ],
   "source": [
    "#The first two models using feature_set 1 and the last two model uses an alternate feature set\n",
    "\n",
    "print(nltk.classify.util.accuracy(classifier_nb, training))\n",
    "print(nltk.classify.util.accuracy(classifier_sk, training))\n",
    "\n",
    "print(nltk.classify.util.accuracy(classifier_sk_alt1, training))\n",
    "print(nltk.classify.util.accuracy(classifier_nb_alt, training))\n",
    "\n",
    "# I choose the NB model with an alternate feature set, which includes prefixes and suffixes as features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Give predictions of test set\n",
    "test=pd.read_csv('test.csv')\n",
    "pred_uname1 = []\n",
    "for i in range(len(test)):\n",
    "    res=(classifier_nb_alt.classify(name_features2(test.username[i])))\n",
    "    pred_uname1.append(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# support your predictions are stored in a list named pred_uname\n",
    "zz = pd.DataFrame({'username':test['username'], 'prediction':pred_uname1})\n",
    "zz.to_csv('vay16001_1.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Predicting Gender with Description\n",
    "The updated notebook for lecture 11 might be of some help, which now includes demo code for making predictions with NLTK classifier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\Varni\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "import string\n",
    "string.punctuation\n",
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "set(stopwords.words('english'))\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer\n",
    "import string\n",
    "ps = PorterStemmer()\n",
    "from nltk.tokenize import word_tokenize\n",
    "import itertools\n",
    "def preprocess(text):\n",
    "    return [ps.stem(w) for w in word_tokenize(text.lower()) \n",
    "             if w not in string.punctuation and w not in stopwords.words('english')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'pd' is not defined",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-2-d971d5064ba7>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mtrain\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'train.csv'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'pd' is not defined"
     ],
     "output_type": "error"
    }
   ],
   "source": [
    "train = pd.read_csv('train.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Defining a function to process sentences and create a corpus of frequent words\n",
    "def split_wordslist(a):\n",
    "    all_words=[]\n",
    "    result = []\n",
    "    for i in range(0,len(a)):\n",
    "            result= preprocess(a.description[i])\n",
    "            all_words.append(result)\n",
    "    all_words_list= list(itertools.chain.from_iterable(all_words))\n",
    "    desc_words= list(zip(all_words,a.gender))\n",
    "    words_freq = nltk.FreqDist(all_words_list)\n",
    "    selected_words = [word for word, freq in words_freq.items() if freq>1]\n",
    "    return desc_words,selected_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "desc_words_train,selected_words_train = split_wordslist(train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "\n",
    "def pos_counts(text):\n",
    "    tagged = nltk.pos_tag(text)\n",
    "    counts = Counter(tag for word,tag in tagged)\n",
    "    total = sum(counts.values())\n",
    "    return dict((word, float(count)/total) for word,count in counts.items())\n",
    "\n",
    "def extract_feature_alt(words):\n",
    "    return pos_counts(words)\n",
    "\n",
    "def extract_features(words, selected_words):\n",
    "    ''' simply using words counts'''\n",
    "    return nltk.FreqDist([w for w in words if w in selected_words])\n",
    "\n",
    "def extract_feature_comb(words,selected_words):\n",
    "    features = {}\n",
    "    x = extract_feature_alt(words)\n",
    "    y = dict(nltk.FreqDist([w for w in words if w in selected_words]))\n",
    "    features = {**x,**y}\n",
    "    return features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Feature set based on freq-dist only\n",
    "feat = [(extract_features(words,selected_words_train), c) for words, c in desc_words_train]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Feature set based on POS tags\n",
    "feat_alt = [(extract_feature_alt(words), c) for words, c in desc_words_train]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Combined feature set\n",
    "feat2 = [(extract_feature_comb(words,selected_words_train), c) for words, c in desc_words_train]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "({'JJ': 0.23076923076923078,\n",
       "  'NN': 0.5384615384615384,\n",
       "  'NNS': 0.07692307692307693,\n",
       "  'VBD': 0.07692307692307693,\n",
       "  'VBP': 0.07692307692307693,\n",
       "  'compani': 2,\n",
       "  'e-learn': 1,\n",
       "  'expertis': 1,\n",
       "  'know': 1,\n",
       "  'media': 1,\n",
       "  'provid': 1,\n",
       "  'servic': 1,\n",
       "  'social': 1,\n",
       "  'solut': 1,\n",
       "  'url': 1,\n",
       "  'visit': 1},\n",
       " 'M')"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "feat2[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy: 0.5576\n",
      "accuracy: 0.5408\n",
      "accuracy: 0.5552\n",
      "accuracy: 0.5632\n",
      "accuracy: 0.5716573258606885\n",
      "CV mean accuracy: 0.557691465172\n",
      "Most Informative Features\n",
      "                  well.i = 1                   F : M      =     19.0 : 1.0\n",
      "                   femal = 1                   F : M      =     19.0 : 1.0\n",
      "                bookkeep = 1                   F : M      =     19.0 : 1.0\n",
      "                    yoga = 1                   F : M      =     16.1 : 1.0\n",
      "                   clerk = 1                   F : M      =     16.1 : 1.0\n"
     ]
    }
   ],
   "source": [
    "#Model 1 - Freq dist\n",
    "import nltk\n",
    "from sklearn.model_selection import KFold\n",
    "import numpy as np\n",
    "k_fold = KFold(n_splits=5, shuffle=True)\n",
    "accu = []\n",
    "for train_idx, test_idx in k_fold.split(feat):\n",
    "    training = [feat[i] for i in train_idx]\n",
    "    testing = [feat[i] for i in test_idx]\n",
    "    classifier2_nb = nltk.NaiveBayesClassifier.train(training)   \n",
    "    accu.append( nltk.classify.util.accuracy(classifier2_nb, testing) )\n",
    "    print('accuracy:', accu[len(accu)-1])    \n",
    "print('CV mean accuracy:', np.mean(accu)) \n",
    "classifier2_nb.show_most_informative_features(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy: 0.7432\n",
      "accuracy: 0.7352\n",
      "accuracy: 0.7424\n",
      "accuracy: 0.7192\n",
      "accuracy: 0.7534027221777422\n",
      "CV mean accuracy: 0.738680544436\n"
     ]
    }
   ],
   "source": [
    "#Model 2 - Freq dist\n",
    "from nltk.classify import SklearnClassifier\n",
    "from sklearn.svm import SVC\n",
    "k_fold = KFold(n_splits=5, shuffle=True)\n",
    "accu = []\n",
    "for train_idx, test_idx in k_fold.split(feat):\n",
    "    train = [feat[i] for i in train_idx]\n",
    "    test = [feat[i] for i in test_idx]\n",
    "    classifier2_sk = SklearnClassifier(SVC(kernel='linear', C=10, random_state=1), sparse=True).train(train)       \n",
    "    accu.append( nltk.classify.util.accuracy(classifier2_sk, test) )\n",
    "    print('accuracy:', accu[len(accu)-1])    \n",
    "print('CV mean accuracy:', np.mean(accu))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy: 0.828\n",
      "accuracy: 0.8192\n",
      "accuracy: 0.8056\n",
      "accuracy: 0.7904\n",
      "accuracy: 0.8214571657325861\n",
      "CV mean accuracy: 0.812931433147\n"
     ]
    }
   ],
   "source": [
    "#Model 3 - Using POS features\n",
    "from nltk.classify import SklearnClassifier\n",
    "from sklearn.svm import SVC\n",
    "k_fold = KFold(n_splits=5, shuffle=True)\n",
    "accu = []\n",
    "for train_idx, test_idx in k_fold.split(feat_alt):\n",
    "    train = [feat_alt[i] for i in train_idx]\n",
    "    test = [feat_alt[i] for i in test_idx]\n",
    "    classifier2_sk_alt = SklearnClassifier(SVC(kernel='linear', C=10, random_state=1), sparse=True).train(train)       \n",
    "    accu.append( nltk.classify.util.accuracy(classifier2_sk_alt, test) )\n",
    "    print('accuracy:', accu[len(accu)-1])    \n",
    "print('CV mean accuracy:', np.mean(accu))\n",
    "\n",
    "#POS features are giving better results with Scikit learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  ==> Training (1 iterations)\n",
      "\n",
      "      Iteration    Log Likelihood    Accuracy\n",
      "      ---------------------------------------\n",
      "             1          -0.69315        0.817\n",
      "         Final          -0.38736        0.805\n",
      "accuracy: 0.7776\n",
      "  ==> Training (1 iterations)\n",
      "\n",
      "      Iteration    Log Likelihood    Accuracy\n",
      "      ---------------------------------------\n",
      "             1          -0.69315        0.815\n",
      "         Final          -0.36019        0.808\n",
      "accuracy: 0.7904\n",
      "  ==> Training (1 iterations)\n",
      "\n",
      "      Iteration    Log Likelihood    Accuracy\n",
      "      ---------------------------------------\n",
      "             1          -0.69315        0.809\n",
      "         Final          -0.41882        0.788\n",
      "accuracy: 0.8112\n",
      "  ==> Training (1 iterations)\n",
      "\n",
      "      Iteration    Log Likelihood    Accuracy\n",
      "      ---------------------------------------\n",
      "             1          -0.69315        0.812\n",
      "         Final          -0.31931        0.809\n",
      "accuracy: 0.8104\n",
      "  ==> Training (1 iterations)\n",
      "\n",
      "      Iteration    Log Likelihood    Accuracy\n",
      "      ---------------------------------------\n",
      "             1          -0.69315        0.811\n",
      "         Final          -0.36376        0.805\n",
      "accuracy: 0.8126501200960768\n",
      "CV mean accuracy: 0.800450024019\n"
     ]
    }
   ],
   "source": [
    "#Model 4 - freq dist\n",
    "k_fold = KFold(n_splits=5, shuffle=True)\n",
    "accu = []\n",
    "for train_idx, test_idx in k_fold.split(feat):\n",
    "    train = [feat[i] for i in train_idx]\n",
    "    test = [feat[i] for i in test_idx]\n",
    "    classifier2_me = nltk.classify.MaxentClassifier.train(train, trace=3, max_iter=1)       \n",
    "    accu.append( nltk.classify.util.accuracy(classifier2_me, test) )\n",
    "    print('accuracy:', accu[len(accu)-1])    \n",
    "print('CV mean accuracy:', np.mean(accu))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Model 5 - using POS features\n",
    "k_fold = KFold(n_splits=5, shuffle=True)\n",
    "accu = []\n",
    "for train_idx, test_idx in k_fold.split(feat_alt):\n",
    "    train = [feat_alt[i] for i in train_idx]\n",
    "    test = [feat_alt[i] for i in test_idx]\n",
    "    classifier2_me_alt = nltk.classify.MaxentClassifier.train(train, trace=3, max_iter=3)       \n",
    "    accu.append( nltk.classify.util.accuracy(classifier2_me_alt, test) )\n",
    "    print('accuracy:', accu[len(accu)-1])    \n",
    "print('CV mean accuracy:', np.mean(accu))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 661,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy: 0.5576\n",
      "accuracy: 0.5752\n",
      "accuracy: 0.5928\n",
      "accuracy: 0.6\n",
      "accuracy: 0.6108887109687751\n",
      "CV mean accuracy: 0.587297742194\n"
     ]
    }
   ],
   "source": [
    "#Model 6 - combined feature set\n",
    "from sklearn.model_selection import KFold\n",
    "import numpy as np\n",
    "k_fold = KFold(n_splits=5, shuffle=True)\n",
    "accu = []\n",
    "for train_idx, test_idx in k_fold.split(feat2):\n",
    "    train = [feat2[i] for i in train_idx]\n",
    "    test = [feat2[i] for i in test_idx]\n",
    "    classifier_nb2_alt = nltk.NaiveBayesClassifier.train(train)   \n",
    "    accu.append( nltk.classify.util.accuracy(classifier_nb2_alt, test) )\n",
    "    print('accuracy:', accu[len(accu)-1])    \n",
    "print('CV mean accuracy:', np.mean(accu))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  ==> Training (5 iterations)\n",
      "\n",
      "      Iteration    Log Likelihood    Accuracy\n",
      "      ---------------------------------------\n",
      "             1          -0.69315        0.810\n",
      "             2          -0.31461        0.811\n",
      "             3          -0.27804        0.811\n",
      "             4          -0.26614        0.806\n",
      "         Final          -0.26087        0.804\n",
      "accuracy: 0.8104\n",
      "  ==> Training (5 iterations)\n",
      "\n",
      "      Iteration    Log Likelihood    Accuracy\n",
      "      ---------------------------------------\n",
      "             1          -0.69315        0.814\n",
      "             2          -0.33384        0.815\n",
      "             3          -0.29014        0.810\n",
      "             4          -0.27847        0.805\n",
      "      Training stopped: keyboard interrupt\n",
      "         Final          -0.27847        0.805\n",
      "accuracy: 0.7936\n",
      "  ==> Training (5 iterations)\n",
      "\n",
      "      Iteration    Log Likelihood    Accuracy\n",
      "      ---------------------------------------\n",
      "             1          -0.69315        0.811\n",
      "             2          -0.33170        0.812\n",
      "      Training stopped: keyboard interrupt\n",
      "         Final          -0.33170        0.812\n",
      "accuracy: 0.82\n",
      "  ==> Training (5 iterations)\n",
      "\n",
      "      Iteration    Log Likelihood    Accuracy\n",
      "      ---------------------------------------\n",
      "             1          -0.69315        0.813\n",
      "      Training stopped: keyboard interrupt\n",
      "         Final          -0.69315        0.813\n",
      "accuracy: 0.812\n",
      "  ==> Training (5 iterations)\n",
      "\n",
      "      Iteration    Log Likelihood    Accuracy\n",
      "      ---------------------------------------\n",
      "             1          -0.69315        0.816\n",
      "      Training stopped: keyboard interrupt\n",
      "         Final          -0.69315        0.816\n",
      "accuracy: 0.7990392313851081\n",
      "CV mean accuracy: 0.807007846277\n"
     ]
    }
   ],
   "source": [
    "#Model 7 - combined feature set\n",
    "from sklearn.model_selection import KFold\n",
    "import numpy as np\n",
    "k_fold = KFold(n_splits=5, shuffle=True)\n",
    "accu = []\n",
    "for train_idx, test_idx in k_fold.split(feat2):\n",
    "    train = [feat2[i] for i in train_idx]\n",
    "    test = [feat2[i] for i in test_idx]\n",
    "    classifier2_me_cm = nltk.classify.MaxentClassifier.train(train, trace=3, max_iter=3)       \n",
    "    accu.append( nltk.classify.util.accuracy(classifier2_me_cm, test) )\n",
    "    print('accuracy:', accu[len(accu)-1])    \n",
    "print('CV mean accuracy:', np.mean(accu))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy: 0.8264\n",
      "accuracy: 0.8232\n",
      "accuracy: 0.8072\n",
      "accuracy: 0.792\n",
      "accuracy: 0.8158526821457166\n",
      "CV mean accuracy: 0.812930536429\n"
     ]
    }
   ],
   "source": [
    "#Model 8 - combined feature set - SKlearn\n",
    "from nltk.classify import SklearnClassifier\n",
    "from sklearn.svm import SVC\n",
    "k_fold = KFold(n_splits=5, shuffle=True)\n",
    "accu = []\n",
    "for train_idx, test_idx in k_fold.split(feat_alt):\n",
    "    train = [feat_alt[i] for i in train_idx]\n",
    "    test = [feat_alt[i] for i in test_idx]\n",
    "    classifier2_sk_alt = SklearnClassifier(SVC(kernel='linear', C=10, random_state=1), sparse=True).train(train)       \n",
    "    accu.append( nltk.classify.util.accuracy(classifier2_sk_alt, test) )\n",
    "    print('accuracy:', accu[len(accu)-1])    \n",
    "print('CV mean accuracy:', np.mean(accu))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 663,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Most Informative Features\n",
      "                   woman = 1                   F : M      =     27.8 : 1.0\n",
      "                   femal = 1                   F : M      =     22.0 : 1.0\n",
      "                     cat = 1                   F : M      =     19.0 : 1.0\n",
      "                   linux = 1                   M : F      =     17.1 : 1.0\n",
      "                     mom = 1                   F : M      =     17.0 : 1.0\n",
      "    don\\xe2\\u20ac\\u2122t = 1                   F : M      =     16.1 : 1.0\n",
      "                 tourist = 1                   F : M      =     16.1 : 1.0\n",
      "                 inquiri = 1                   F : M      =     16.1 : 1.0\n",
      "               pre-press = 1                   F : M      =     16.1 : 1.0\n",
      "                     VBN = 0.011904761904761904      F : M      =     14.5 : 1.0\n",
      "                   beach = 1                   F : M      =     13.2 : 1.0\n",
      "  master\\xe2\\u20ac\\u2122 = 1                   F : M      =     13.2 : 1.0\n",
      "                   funni = 1                   F : M      =     13.2 : 1.0\n",
      "             autorespond = 1                   F : M      =     13.2 : 1.0\n",
      "                   album = 1                   F : M      =     13.2 : 1.0\n"
     ]
    }
   ],
   "source": [
    "classifier_nb2_alt.show_most_informative_features(15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Test - predictions - Choosing Entropy classifier with combined feature set\n",
    "test = pd.read_csv('test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_words_test=[]\n",
    "for i in range(0,len(test)):\n",
    "    result= preprocess(test.description[i])\n",
    "    all_words_test.append(result)\n",
    "\n",
    "import itertools\n",
    "all_words_list_test= list(itertools.chain.from_iterable(all_words_test))\n",
    "desc_words_test= list(all_words_test)\n",
    "desc_test_df=pd.DataFrame(desc_words_test)\n",
    "words_freq_test = nltk.FreqDist(all_words_list_test)\n",
    "\n",
    "selected_words_test = [word for word, freq in words_freq_test.items() if freq>1]\n",
    "feat_test = [(extract_features(words1,selected_words_test)) for words1 in desc_words_test]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "FreqDist({'16': 1,\n",
       "          'articl': 1,\n",
       "          'blog': 1,\n",
       "          'busi': 1,\n",
       "          'comput': 2,\n",
       "          'correspond': 1,\n",
       "          'design': 1,\n",
       "          'develop': 2,\n",
       "          'etc': 1,\n",
       "          'experi': 1,\n",
       "          'one': 1,\n",
       "          'page': 2,\n",
       "          'person': 1,\n",
       "          'program': 1,\n",
       "          'self-employ': 1,\n",
       "          'softwar': 1,\n",
       "          'web': 2,\n",
       "          'write': 1,\n",
       "          'year': 1})"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "feat_test[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Sk classifier with freq-dist feature set\n",
    "pred_uname2 = [classifier2_sk.classify(row) for row in feat_test]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "test=pd.read_csv[]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "zz = pd.DataFrame({'username':test['username'], 'prediction':pred_uname2})\n",
    "zz.to_csv('vay16001_2.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Choosing Entropy classifier with POS tags as feature set\n",
    "\n",
    "pred_uname3 = [classifier2_me.classify(row) for row in feat_test]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "zz = pd.DataFrame({'username':test['username'], 'prediction':pred_uname3})\n",
    "zz.to_csv('vay16001_3.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Predicting Gender with Username, Description, and Status\n",
    "If you need to merge multiple dict-format features into one, check the following question: https://stackoverflow.com/questions/38987/how-to-merge-two-dictionaries-in-a-single-expression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train = pd.read_csv('train.csv')\n",
    "test = pd.read_csv('test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Parse Json format status as dictionary\n",
    "from ast import literal_eval\n",
    "status = train['status'].apply(literal_eval)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "t_status = test['status'].apply(literal_eval)\n",
    "t_status=t_status.apply(pd.Series)\n",
    "test = pd.concat([test,t_status], axis=1,join='inner')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>username</th>\n",
       "      <th>gender</th>\n",
       "      <th>status</th>\n",
       "      <th>description</th>\n",
       "      <th>deposit_made</th>\n",
       "      <th>email_verified</th>\n",
       "      <th>facebook_connected</th>\n",
       "      <th>identity_verified</th>\n",
       "      <th>payment_verified</th>\n",
       "      <th>phone_verified</th>\n",
       "      <th>profile_complete</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Vimal20011</td>\n",
       "      <td>M</td>\n",
       "      <td>{u'payment_verified': False, u'identity_verifi...</td>\n",
       "      <td>A team of 5 working on various projects relate...</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>sheom</td>\n",
       "      <td>M</td>\n",
       "      <td>{u'payment_verified': True, u'identity_verifie...</td>\n",
       "      <td>We are an IT solution and service provider com...</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>ezbik</td>\n",
       "      <td>M</td>\n",
       "      <td>{u'payment_verified': False, u'identity_verifi...</td>\n",
       "      <td>System administration is my work &amp; hobby.</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>angelme</td>\n",
       "      <td>F</td>\n",
       "      <td>{u'payment_verified': False, u'identity_verifi...</td>\n",
       "      <td>Good day! Thank you for taking some time to ch...</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>snitch1</td>\n",
       "      <td>M</td>\n",
       "      <td>{u'payment_verified': False, u'identity_verifi...</td>\n",
       "      <td>I build good relation with clients and deliver...</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     username gender                                             status  \\\n",
       "0  Vimal20011      M  {u'payment_verified': False, u'identity_verifi...   \n",
       "1       sheom      M  {u'payment_verified': True, u'identity_verifie...   \n",
       "2       ezbik      M  {u'payment_verified': False, u'identity_verifi...   \n",
       "3     angelme      F  {u'payment_verified': False, u'identity_verifi...   \n",
       "4     snitch1      M  {u'payment_verified': False, u'identity_verifi...   \n",
       "\n",
       "                                         description  deposit_made  \\\n",
       "0  A team of 5 working on various projects relate...          True   \n",
       "1  We are an IT solution and service provider com...          True   \n",
       "2          System administration is my work & hobby.         False   \n",
       "3  Good day! Thank you for taking some time to ch...          True   \n",
       "4  I build good relation with clients and deliver...         False   \n",
       "\n",
       "   email_verified  facebook_connected  identity_verified  payment_verified  \\\n",
       "0            True               False              False             False   \n",
       "1            True                True              False              True   \n",
       "2            True               False              False             False   \n",
       "3            True               False              False             False   \n",
       "4            True               False              False             False   \n",
       "\n",
       "   phone_verified  profile_complete  \n",
       "0           False              True  \n",
       "1           False              True  \n",
       "2           False              True  \n",
       "3            True              True  \n",
       "4           False              True  "
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Now you need to find a way to split the dictionary format status as multiple columns\n",
    "status=status.apply(pd.Series)\n",
    "train = pd.concat([train,status], axis=1,join='inner')\n",
    "train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Data Preprocessing\n",
    "\n",
    "train_list=list(zip(train['username'],train['description'],train['email_verified'],\n",
    "                   train['facebook_connected'],train['identity_verified'],train['payment_verified'],train['phone_verified'],\n",
    "                    train['profile_complete']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_list=list(zip(test['username'],test['description'],test['email_verified'],\n",
    "                   test['facebook_connected'],test['identity_verified'],test['payment_verified'],test['phone_verified'],\n",
    "                    test['profile_complete']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_list=list(zip(train_list,train['gender']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Using freq-dist - extract_features\n",
    "def all_feat(lista,selected_words_train):\n",
    "    f1 = name_features(lista[0][0])\n",
    "    f2 = extract_features(lista[1],selected_words_train)\n",
    "    f3 = dict(zip(('a','b','c','d','e'), lista[2:9]))\n",
    "    all_f = {**f1,**f2,**f3}\n",
    "    print(lista[0][0])\n",
    "    return all_f\n",
    "\n",
    "# Using POS tags - extract_features_alt\n",
    "def all_feat_alt(lista,selected_words_train):\n",
    "    f1 = name_features(lista[0])\n",
    "    f2 = extract_feature_alt(lista[1])\n",
    "    f3 = dict(zip(('a','b','c','d','e'), lista[2:9]))\n",
    "    all_f = {**f1,**f2,**f3}\n",
    "    return all_f"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "feat_set_f = [(all_feat(item,selected_words_train),gender) for (item,gender) in train_list]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "metadata": {},
   "outputs": [],
   "source": [
    "def all_feat(lista,selected_words_train):\n",
    "    f1 = name_features(lista[0])\n",
    "    \n",
    "    f2 = extract_features(lista[1],selected_words_train)\n",
    "    f3 = dict(zip(('a','b','c','d','e'), lista[2:9]))\n",
    "    all_f = {**f1,**f2,**f3}\n",
    "    print(lista[1])\n",
    "    #print(f2)\n",
    "    #print(f3)\n",
    "    return lista[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "We are an IT solution and service provider company having expertise in e-learning and social media marketing.please visit out company URLs to know more:\n"
     ]
    }
   ],
   "source": [
    "a=train_list[1:2]\n",
    "b=[(all_feat(item,selected_words_train),gender) for (item,gender) in a]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "feat_set_f_alt = [(all_feat_alt(item,selected_words_train),gender) for (item,gender) in train_list]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "feat_set_test_alt = [all_feat_alt(item,selected_words_test) for  item in test_list]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "feat_set_test =[all_feat(item,selected_words_test) for  item in test_list]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'first_letter': 'n', 'last_letter': 'a', 'upper_cnt': 0, 'lower_cnt': 12, 'hasupper': False, 'length': 12, 'vowel_count': 5, 'digits_in': 0, 'endswithvowel': True, 'n': 15, 'e': False, 'f': 3, 'l': 6, 'p': 11, 'r': 13, 'h': 2, 'v': 3, 'g': 10, '1': 1, '6': 1, 'x': 1, 'c': False, 'u': 3, 'w': 2, 'b': False, 'a': True, 'd': False}\n",
      "{'first_letter': 'n', 'last_letter': 'a', 'upper_cnt': 0, 'lower_cnt': 12, 'hasupper': False, 'length': 12, 'vowel_count': 5, 'digits_in': 0, 'endswithvowel': True, 'PRP': 0.004524886877828055, 'VBP': 0.058823529411764705, 'DT': 0.03619909502262444, 'NN': 0.5746606334841629, 'NNP': 0.14027149321266968, 'VBZ': 0.03167420814479638, 'JJ': 0.06787330316742081, 'SYM': 0.004524886877828055, ':': 0.004524886877828055, 'VBD': 0.004524886877828055, 'CD': 0.00904977375565611, ',': 0.03619909502262444, 'MD': 0.004524886877828055, 'VB': 0.00904977375565611, 'IN': 0.004524886877828055, 'FW': 0.004524886877828055, '.': 0.004524886877828055, 'a': True, 'b': False, 'c': False, 'd': False, 'e': False}\n"
     ]
    }
   ],
   "source": [
    "#This set uses freq dist for description\n",
    "print(feat_set_test[0] )\n",
    "#This set uses POS tags for description\n",
    "print(feat_set_test_alt[0] )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy: 0.7696\n",
      "accuracy: 0.7432\n",
      "accuracy: 0.7824\n",
      "accuracy: 0.7712\n",
      "accuracy: 0.7485988791032826\n",
      "CV mean accuracy: 0.762999775821\n"
     ]
    }
   ],
   "source": [
    "#Naive Bayes for feature set 1 (freq dist)\n",
    "from sklearn.model_selection import KFold\n",
    "import numpy as np\n",
    "k_fold = KFold(n_splits=5, shuffle=True)\n",
    "accu = []\n",
    "for train_idx, test_idx in k_fold.split(feat_set_f):\n",
    "    training_3= [feat_set_f[i] for i in train_idx]\n",
    "    testing_3 = [feat_set_f[i] for i in test_idx]\n",
    "    classifier3_nb = nltk.NaiveBayesClassifier.train(training_3)   \n",
    "    accu.append( nltk.classify.util.accuracy(classifier3_nb, testing_3) )\n",
    "    print('accuracy:', accu[len(accu)-1])    \n",
    "print('CV mean accuracy:', np.mean(accu))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy: 0.7896\n",
      "accuracy: 0.8016\n",
      "accuracy: 0.8104\n",
      "accuracy: 0.8112\n",
      "accuracy: 0.7854283426741393\n",
      "CV mean accuracy: 0.799645668535\n"
     ]
    }
   ],
   "source": [
    "#Naive Bayes for feature set 2 (freq dist)\n",
    "from sklearn.model_selection import KFold\n",
    "import numpy as np\n",
    "k_fold = KFold(n_splits=5, shuffle=True)\n",
    "accu = []\n",
    "for train_idx, test_idx in k_fold.split(feat_set_f_alt):\n",
    "    training_3= [feat_set_f_alt[i] for i in train_idx]\n",
    "    testing_3 = [feat_set_f_alt[i] for i in test_idx]\n",
    "    classifier3_nb_alt = nltk.NaiveBayesClassifier.train(training_3)   \n",
    "    accu.append( nltk.classify.util.accuracy(classifier3_nb, testing_3) )\n",
    "    print('accuracy:', accu[len(accu)-1])    \n",
    "print('CV mean accuracy:', np.mean(accu))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Most Informative Features\n",
      "                       h = 39                  F : M      =     18.0 : 1.0\n",
      "                       k = 32                  F : M      =     11.1 : 1.0\n",
      "                       f = 29                  F : M      =     10.9 : 1.0\n",
      "                       2 = 29                  F : M      =      9.8 : 1.0\n",
      "             last_letter = 'B'                 F : M      =      9.3 : 1.0\n",
      "                       w = 26                  F : M      =      9.3 : 1.0\n",
      "                       z = 8                   F : M      =      7.8 : 1.0\n",
      "            first_letter = 'Y'                 F : M      =      7.7 : 1.0\n",
      "                       4 = 7                   F : M      =      7.2 : 1.0\n",
      "                       8 = 15                  F : M      =      7.2 : 1.0\n",
      "                       3 = 21                  F : M      =      7.2 : 1.0\n",
      "                       9 = 13                  F : M      =      7.2 : 1.0\n",
      "                       1 = 18                  F : M      =      7.1 : 1.0\n",
      "                       k = 54                  F : M      =      7.1 : 1.0\n",
      "            first_letter = 'U'                 F : M      =      7.1 : 1.0\n",
      "                       v = 36                  F : M      =      7.0 : 1.0\n",
      "                       f = 46                  F : M      =      7.0 : 1.0\n",
      "                       f = 37                  F : M      =      7.0 : 1.0\n",
      "                       g = 75                  F : M      =      6.9 : 1.0\n",
      "                       g = 48                  F : M      =      6.9 : 1.0\n",
      "                       h = 54                  F : M      =      6.9 : 1.0\n",
      "                       h = 58                  F : M      =      6.9 : 1.0\n",
      "                       u = 53                  F : M      =      6.9 : 1.0\n",
      "                       u = 58                  F : M      =      6.9 : 1.0\n",
      "                       l = 57                  F : M      =      6.8 : 1.0\n",
      "Most Informative Features\n",
      "                      CC = 0.004291845493562232      F : M      =      8.7 : 1.0\n",
      "                       $ = 0.006578947368421052      F : M      =      8.2 : 1.0\n",
      "                      VB = 0.034482758620689655      F : M      =      8.1 : 1.0\n",
      "                       , = 0.02586206896551724      F : M      =      7.6 : 1.0\n",
      "                      JJ = 0.09322033898305085      F : M      =      7.1 : 1.0\n"
     ]
    }
   ],
   "source": [
    "classifier3_nb.show_most_informative_features(25)\n",
    "classifier3_nb_alt.show_most_informative_features(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 515,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  ==> Training (5 iterations)\n",
      "\n",
      "      Iteration    Log Likelihood    Accuracy\n",
      "      ---------------------------------------\n",
      "             1          -0.69315        0.816\n",
      "             2          -0.35201        0.816\n",
      "             3          -0.34493        0.816\n",
      "             4          -0.33917        0.818\n",
      "         Final          -0.33412        0.821\n",
      "accuracy: 0.7992\n",
      "  ==> Training (5 iterations)\n",
      "\n",
      "      Iteration    Log Likelihood    Accuracy\n",
      "      ---------------------------------------\n",
      "             1          -0.69315        0.812\n",
      "             2          -0.35916        0.812\n",
      "             3          -0.35154        0.812\n",
      "             4          -0.34534        0.815\n",
      "         Final          -0.33994        0.818\n",
      "accuracy: 0.816\n",
      "  ==> Training (5 iterations)\n",
      "\n",
      "      Iteration    Log Likelihood    Accuracy\n",
      "      ---------------------------------------\n",
      "             1          -0.69315        0.812\n",
      "             2          -0.35911        0.812\n",
      "             3          -0.35225        0.813\n",
      "             4          -0.34665        0.814\n",
      "         Final          -0.34172        0.818\n",
      "accuracy: 0.8152\n",
      "  ==> Training (5 iterations)\n",
      "\n",
      "      Iteration    Log Likelihood    Accuracy\n",
      "      ---------------------------------------\n",
      "             1          -0.69315        0.812\n",
      "             2          -0.36025        0.812\n",
      "             3          -0.35333        0.812\n",
      "             4          -0.34776        0.814\n",
      "         Final          -0.34293        0.818\n",
      "accuracy: 0.8184\n",
      "  ==> Training (5 iterations)\n",
      "\n",
      "      Iteration    Log Likelihood    Accuracy\n",
      "      ---------------------------------------\n",
      "             1          -0.69315        0.812\n",
      "             2          -0.35937        0.812\n",
      "             3          -0.35207        0.813\n",
      "             4          -0.34616        0.815\n",
      "      Training stopped: keyboard interrupt\n",
      "         Final          -0.34616        0.815\n",
      "accuracy: 0.8158526821457166\n",
      "CV mean accuracy: 0.812930536429\n"
     ]
    }
   ],
   "source": [
    "k_fold = KFold(n_splits=5, shuffle=True)\n",
    "accu = []\n",
    "for train_idx, test_idx in k_fold.split(feat_set_f):\n",
    "    training_3me = [feat_set_f[i] for i in train_idx]\n",
    "    testing_3me = [feat_set_f[i] for i in test_idx]\n",
    "    classifier3_me = nltk.classify.MaxentClassifier.train(training_3me, trace=3, max_iter=5)       \n",
    "    accu.append( nltk.classify.util.accuracy(classifier3_me, testing_3me) )\n",
    "    print('accuracy:', accu[len(accu)-1])    \n",
    "print('CV mean accuracy:', np.mean(accu))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  ==> Training (1 iterations)\n",
      "\n",
      "      Iteration    Log Likelihood    Accuracy\n",
      "      ---------------------------------------\n",
      "             1          -0.69315        0.811\n",
      "         Final          -0.35237        0.811\n",
      "accuracy: 0.8216\n",
      "  ==> Training (1 iterations)\n",
      "\n",
      "      Iteration    Log Likelihood    Accuracy\n",
      "      ---------------------------------------\n",
      "             1          -0.69315        0.809\n",
      "         Final          -0.35416        0.809\n",
      "accuracy: 0.8296\n",
      "  ==> Training (1 iterations)\n",
      "\n",
      "      Iteration    Log Likelihood    Accuracy\n",
      "      ---------------------------------------\n",
      "             1          -0.69315        0.815\n",
      "         Final          -0.34509        0.815\n",
      "accuracy: 0.8032\n",
      "  ==> Training (1 iterations)\n",
      "\n",
      "      Iteration    Log Likelihood    Accuracy\n",
      "      ---------------------------------------\n",
      "             1          -0.69315        0.813\n",
      "         Final          -0.34841        0.813\n",
      "accuracy: 0.8128\n",
      "  ==> Training (1 iterations)\n",
      "\n",
      "      Iteration    Log Likelihood    Accuracy\n",
      "      ---------------------------------------\n",
      "             1          -0.69315        0.817\n",
      "         Final          -0.34316        0.817\n",
      "accuracy: 0.7974379503602882\n",
      "CV mean accuracy: 0.812927590072\n"
     ]
    }
   ],
   "source": [
    "k_fold = KFold(n_splits=5, shuffle=True)\n",
    "accu = []\n",
    "for train_idx, test_idx in k_fold.split(feat_set_f_alt):\n",
    "    training_3me = [feat_set_f_alt[i] for i in train_idx]\n",
    "    testing_3me = [feat_set_f_alt[i] for i in test_idx]\n",
    "    classifier3_me_alt = nltk.classify.MaxentClassifier.train(training_3me, trace=3, max_iter=1)       \n",
    "    accu.append( nltk.classify.util.accuracy(classifier3_me, testing_3me) )\n",
    "    print('accuracy:', accu[len(accu)-1])    \n",
    "print('CV mean accuracy:', np.mean(accu))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pred_uname4 = [classifier3_me.classify(row) for row in feat_set_test]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_uname5 = [classifier3_me_alt.classify(row) for row in feat_set_test_alt]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "zz = pd.DataFrame({'username':test['username'], 'prediction':pred_uname4})\n",
    "zz.to_csv('vay16001_4.csv', index=False)\n",
    "\n",
    "zz = pd.DataFrame({'username':test['username'], 'prediction':pred_uname5})\n",
    "zz.to_csv('vay16001_5.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extra Credit: Try Different Features and Models for Best Performance\n",
    "Save your predictions as netid_1.csv, ..., netid_5.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Predictions from different feature sets and models have been saved under each sub question\n",
    "# One is using user only; two submissions are for description; two submissions using username, description, status\n",
    "User only - \n",
    "I chose the feature set including prefixes and suffixes as I felt this was a more intelligent feature selection.\n",
    "\n",
    "Description - \n",
    "I tried three types of feature sets - Freq dist, POS tags, combination of both\n",
    "Though I got good results by using POS tags in my model trials, the function was not working for the test csv.\n",
    "I failed to debug this in time so both predictions are based on FreqDist for two diff classifiers.\n",
    "\n",
    "Username, Description, Status- \n",
    "\n",
    "I had two features sets - one that used freq dist for and one that used POS tags for description field. \n",
    "Features derived from username and status were common for both.\n",
    "I have two sets of predictions from the maximum entropy classifier for both."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3.0
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
